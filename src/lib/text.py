def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:

    if casefold==True:
        text=text.casefold()
        #–ø—Ä–∏–≤–æ–∂—É –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        
    if yo2e==True:
        text=text.replace('—ë','–µ').replace('–Å','–ï')
        #–∑–∞–º–µ–Ω—è—é —ë/–Å

    text=text.replace('\n',' ').replace('\t',' ').replace('\r',' ')
    #–∑–∞–º–µ–Ω—è—é —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
    
    text=text.split() #—Ä–∞–∑–±–∏–≤–∞—é –ø–æ –ø—Ä–æ–±–µ–ª—É (–ø–æ–ª—É—á—É —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤)

    text=' '.join(text) #—Å–æ–µ–¥–∏–Ω—è—é —ç–¥–µ–º–µ–Ω—Ç—ã –≤ —Å–ø–∏—Å–∫–µ(—Å–ª–æ–≤–∞) —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª

    return text
print(normalize("Hello\r\nWorld"))



import re

def tokenize(text: str) -> list[str]:

    pattern= r'\b\w+(?:-\w+)*\b'

    tokens=re.findall(pattern,text)

    return tokens
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))



def count_freq(tokens: list[str]) -> dict[str, int]:

    word_count={}

    for word in tokens:
        word_count[word]=word_count.get(word,0)+1 #–µ—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, —Ç–æ get –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –µ—Å–ª–∏ –Ω–µ—Ç, —Ç–æ 0

    return(word_count)
print(count_freq(["bb","aa","bb","aa","cc"]))



def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:

    sorted_word=sorted(freq.items(), key=lambda x: (x[0])) #—Å–æ—Ä—Ç–∏—Ä—É—é –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    sorted_word=sorted(freq.items(), key=lambda x: (x[1]),reverse=1) #—Å–æ—Ä—Ç–∏—Ä—É—é –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ


    return sorted_word[:n]

tokens=["bb","aa","bb","aa","cc"]

freq_dict = count_freq(tokens)

result = top_n(freq_dict, 2)

print(result)



import sys #—á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –≤–≤–æ–¥ 
#from src.lib.text import normalize, tokenize, count_freq, top_n

def main():

    text=sys.stdin.read()#—á–∏—Ç–∞—é –≤–µ—Å—å –≤–≤–æ–¥ –¥–æ EOF (ctr+Z+En)
    if not text: #–µ—Å–ª–∏ –Ω–µ—Ç –Ω–∏—á–µ–≥–æ –Ω–∞ –≤—Ö–æ–¥–µ
        return "—Ç–µ–∫—Å—Ç –Ω–µ –≤–∏–¥–µ–Ω"

    normalized_text = normalize(text)
    tokens = tokenize(normalized_text)
    count_word = count_freq(tokens)
    top_words = top_n(count_word, 5)
    print("–í—Å–µ–≥–æ —Å–ª–æ–≤:", len(tokens))
    print("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤:", len(count_word))
    print('–¢–æ–ø-5:')
    for word, count in top_words:
        print(f'{word}:{count}')
        
print(main())