def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:

    if casefold==True:
        text=text.casefold()
        #–ø—Ä–∏–≤–æ–∂—É –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        
    if yo2e==True:
        text=text.replace('—ë','–µ').replace('–Å','–ï')
        #–∑–∞–º–µ–Ω—è—é —ë/–Å

    text=text.replace('\n',' ').replace('\t',' ').replace('\r',' ')
    #–∑–∞–º–µ–Ω—è—é —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã
    
    text=text.split() #—Ä–∞–∑–±–∏–≤–∞—é –ø–æ –ø—Ä–æ–±–µ–ª—É (–ø–æ–ª—É—á—É —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤)

    text=' '.join(text) #—Å–æ–µ–¥–∏–Ω—è—é —ç–¥–µ–º–µ–Ω—Ç—ã –≤ —Å–ø–∏—Å–∫–µ(—Å–ª–æ–≤–∞) —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª

    return text




import re

def tokenize(text: str) -> list[str]:

    pattern= r'\w+(?:-\w+)*'

    tokens=re.findall(pattern,text)

    return tokens
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))




def count_freq(tokens: list[str]) -> dict[str, int]:

    word_count={}

    for word in tokens:
        word_count[word]=word_count.get(word,0)+1 #–µ—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ, —Ç–æ get –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –µ—Å–ª–∏ –Ω–µ—Ç, —Ç–æ 0

    return(word_count)




def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:

    sorted_word=sorted(freq.items(), key=lambda x: (x[0])) #—Å–æ—Ä—Ç–∏—Ä—É—é –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
    sorted_word=sorted(freq.items(), key=lambda x: (x[1]),reverse=1) #—Å–æ—Ä—Ç–∏—Ä—É—é –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ


    return sorted_word[:n]

tokens=["bb","aa","bb","aa","cc"]

freq_dict = count_freq(tokens)

result = top_n(freq_dict, 2)

